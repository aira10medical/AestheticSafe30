# ruff: noqa
# pyright: reportUnusedExpression=false

"""
app.py ‚Äî Shell m√≠nimo para AestheticSafe en Streamlit/Railway

- Mantiene toda la l√≥gica actual en calculadora.calculadora()
- Agrega un panel de chat IA a la derecha (SAFE¬∑MD Chat)
- No toca la implementaci√≥n interna de calculadora.py
"""

import os
import streamlit as st
from openai import OpenAI
from calculadora import calculadora  # ‚¨ÖÔ∏è tu app original, intacta

# ==========================
# üîë Cliente OpenAI
# ==========================
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))


# ==========================
# üß† L√≥gica de chat IA
# ==========================
def _ensure_chat_state():
    """Inicializa el historial de chat en sesi√≥n."""
    if "safe_chat_history" not in st.session_state:
        st.session_state["safe_chat_history"] = []  # lista de dicts {role, content}
    if "safe_chat_files" not in st.session_state:
        st.session_state["safe_chat_files"] = []


def _call_safe_md_assistant(question: str, files_context: str = "") -> str:
    """
    Llama a GPT-5.1-mini con un prompt m√©dico controlado.
    No da √≥rdenes m√©dicas directas, responde en lenguaje claro y prudente.
    """
    base_prompt = (
        "Actu√°s como un asistente m√©dico virtual especializado en cirug√≠a pl√°stica est√©tica. "
        "Respond√© en espa√±ol, con lenguaje claro, emp√°tico y profesional. "
        "No realices diagn√≥sticos definitivos ni indiqu√©s tratamientos concretos; "
        "enfatiz√° siempre que la evaluaci√≥n final requiere consulta presencial con el cirujano.\n\n"
    )

    if files_context:
        base_prompt += f"Informaci√≥n sobre archivos adjuntos del paciente:\n{files_context}\n\n"

    full_input = (
        base_prompt
        + "Pregunta actual del paciente:\n"
        + question
    )

    try:
        response = client.responses.create(
            model="gpt-5.1-mini",
            input=full_input,
        )
        return response.output_text
    except Exception as e:
        # Falla segura: no rompe la app, solo informa el error gen√©rico
        return (
            "Hubo un problema al consultar el asistente de IA. "
            "Por favor, intent√° de nuevo m√°s tarde. Detalle t√©cnico: "
            f"{type(e).__name__}"
        )


def render_safe_chat_sidebar():
    """
    Renderiza el m√≥dulo de chat en la columna derecha,
    estilo 'panel Copilot' / mensajer√≠a limpia.
    """
    _ensure_chat_state()

    st.markdown("### ü§ñ SAFE¬∑MD Chat")
    st.caption(
        "Asistente IA para dudas generales sobre cirug√≠a pl√°stica est√©tica. "
        "No reemplaza una consulta m√©dica presencial."
    )

    # ---- Archivos adjuntos ----
    uploaded_files = st.file_uploader(
        "Adjuntar estudios, fotos o documentos (opcional)",
        type=["pdf", "jpg", "jpeg", "png", "dcm", "dicom"],
        accept_multiple_files=True,
        key="safe_chat_files_uploader",
    )

    # Guardamos los archivos s√≥lo en sesi√≥n (no en disco)
    if uploaded_files is not None:
        # streamlit devuelve una lista o [] ‚Äî nos aseguramos de usarla tal cual
        st.session_state["safe_chat_files"] = uploaded_files

    files = st.session_state.get("safe_chat_files", [])
    if files:
        with st.expander("Archivos adjuntados", expanded=False):
            for f in files:
                st.markdown(f"- `{{f.name}}`")

    st.markdown("---")

    # ---- Historial de conversaci√≥n ----
    history = st.session_state["safe_chat_history"]

    # Mostramos √∫ltimos turnos (para no llenar toda la pantalla)
    max_turns = 8
    history_to_show = history[-max_turns:]

    for turn in history_to_show:
        role = turn.get("role", "user")
        content = turn.get("content", "")

        if role == "user":
            st.markdown(f"**T√∫:** {{content}}")
        else:
            # assistant
            st.markdown(f"**SAFE¬∑MD:** {{content}}")

        st.markdown("---")

    # ---- Input de usuario ----
    st.markdown("#### Escrib√≠ tu pregunta")

    with st.form("safe_chat_form", clear_on_submit=True):
        question = st.text_area(
            "Pregunta para SAFE¬∑MD",
            placeholder="Ej: ¬øQu√© significa tener riesgo moderado en mi caso?",
            height=90,
            label_visibility="collapsed",
        )
        send = st.form_submit_button("üí¨ Enviar")

    if send:
        question_stripped = question.strip()
        if not question_stripped:
            st.warning("Escrib√≠ una pregunta antes de enviar.")
            return

        # Registramos turno del usuario
        history.append({"role": "user", "content": question_stripped})

        # Construimos contexto de archivos (por ahora solo nombres)
        files = st.session_state.get("safe_chat_files", [])
        if files:
            files_context = "Archivos adjuntos del paciente:\n" + "\n".join(
                f"- {{f.name}}" for f in files
            )
        else:
            files_context = ""

        with st.spinner("SAFE¬∑MD est√° analizando tu pregunta‚Ä¶"):
            answer = _call_safe_md_assistant(question_stripped, files_context)

        # Registramos respuesta
        history.append({"role": "assistant", "content": answer})
        st.session_state["safe_chat_history"] = history

        # Mostramos respuesta inmediatamente
        st.success("Respuesta de SAFE¬∑MD:")
        st.write(answer)


# ==========================
# üß± Layout principal
# ==========================
def main():
    st.set_page_config(
        page_title="AestheticSafe ¬∑ SAFE¬∑MD",
        page_icon="üíé",
        layout="wide",
    )

    # Layout tipo Copilot: izquierda app m√©dica, derecha chat IA
    col_app, col_chat = st.columns([2.2, 1])

    with col_app:
        # ‚¨áÔ∏è Tu app actual, sin tocar calculadora.py
        calculadora()

    with col_chat:
        render_safe_chat_sidebar()


# Para ejecuci√≥n con `python app.py` o herramientas que esperan entrypoint
if __name__ == "__main__":
    main()


from safe_chat_bottom import render_safe_chat
render_safe_chat()